%template: https://www.sharelatex.com/templates/58d81eaaca5a6fd13992f8a4
\documentclass[12pt,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[style=numeric]{biblatex}
\addbibresource{references.bib}
\usepackage{csquotes}
\usepackage{caption}
\usepackage{python_code}
\usepackage{enumitem}   
\usepackage{pdfpages}
\usepackage{booktabs,xltabular}
\usepackage[dvipsnames]{xcolor}

\usepackage{fancyvrb}
\usepackage{diagbox}
\usepackage{float}
% Include other packages here, before hyperref.

\newenvironment{myitemize}
{ \begin{itemize}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}     }
{ \end{itemize}                  } 

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage[title]{appendix}
\usepackage{xcolor}
\newcommand\myworries[1]{\textcolor{red}{#1}}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\linespread{1.5}
\setlength{\parindent}{2.5em}


% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}
\pagenumbering{roman}
%%%%%%%%% TITLE
\thispagestyle{empty}
\includepdf[pages=-,pagecommand={},width=\paperwidth]{thesis_cover_blank_title.pdf}
\clearpage
\tableofcontents
\newpage

\listoffigures
\newpage 

\pagenumbering{arabic}
%%%%%%%%% ABSTRACT
\section{Abstract}
Machine learning is developing at an unprecedented pace, with applications to everything from self driving cars to medical imaging. Neural networks are the backbone of many of these machine learning implementations due to their inherent ability to learn from input data. Creating an effective machine learning model requires substantial training, which is typically done on Graphics Processing Units (GPUs). Training performance can be improved by loading large amounts of input data onto the GPU concurrently, which requires a large memory footprint. By developing tools to understand how a model is using GPU memory, researchers can gain insight into how to optimize their algorithms to work better on their hardware. This paper presents a novel memory profiler for PyTorch, a popular new machine learning framework. This new profiler's design is a departure from existing profilers, which were designed for frameworks with static computational graphs. The  profiler is an easy-to-use, flexible, low-overhead, and effective solution which decomposes the memory consumption into feature maps, weights, and gradients. It also presents detailed layer-by-layer metrics for further insight into how a model uses memory. The profiler requires minimal setup and can be integrated with existing PyTorch programs using only a few lines of code. Experiments were conducted on existing models such as Neural Collaborative Filtering and ResNet-50, and show the profiler's ability to correctly measure their memory consumption.
\par 


%%%%%%%%% INTRODUCTION
\section{Introduction}
Deep neural networks (DNNs) have a history dating back over half a century, with Donald Hebb's \enquote*{Hebbian Learning Rule} laying the foundations for modern techniques \cite{dnn_history}. In the proceeding years this approach was developed and improved, with notable developments by Rosenblatt (the first perceptron), Werbos (backpropagation), Jordan (Recurrent Neural Network), Hochreiter \& Schmidhuber (LSTM), and Hinton (Deep Belief Networks) \cite{dnn_history}. While this theoretical progression was occurring, the computational machinery required to actually implement these models was non-existent. In the last decade, the development of high-performance computing hardware - especially GPUs - has enabled many of these once-theoretical DNNs to be implemented in practice. While GPUs were once used for graphics processing applications - improving performance by parallelizing computation across many cores - they have found use in machine learning applications. Still, however, there remain hardware bottlenecks which constrain the training performance of machine learning models, and therefore gaining deeper insight into their underlying physical resource usage is vital for further advancements in the field. Given its scarcity, GPU memory is among the most important to understand among these resources. For reference, the NVIDIA RTX 2080 Ti has 11 GB of device memory; even this modern, high-end device will run out of memory during training if the mini-batch size and the network topology are too large. Notable examples of memory optimization techniques that researchers have developed include Gist \cite{gist}, vDNN \cite{vdnn}, SCNN \cite{scnn}, and EIE \cite{eie}. Without profiling tools to gain deeper insight into performance, such advancements would be more difficult. \par

The EcoSystem research group at the University of Toronto has developed open-source memory profiling tools for several machine learning frameworks, including MXNet \cite{mxnet_profiler}, TensorFlow \cite{tensorflow}, and CNTK \cite{cntk}. These tools were created by directly logging all GPU memory allocations made by the framework. Most such allocations happen at the start of the program when the static computational graph (representing the training algorithm) is being created. The profiler then links these allocations to the part of the computational graph which they correspond to, making it possible to create a detailed memory breakdown of the model. The difficulty with creating a memory profiler for PyTorch lies in the fact that PyTorch \textit{(a)} uses \textit{dynamic} computational graphs, one of which is created every iteration of training, and \textit{(b)} incrementally allocates large chunks of GPU memory to create a \textit{cache}. Throughout training, memory is continually being taken and released from the cache as the forward and backward passes progress. \par 

To profile memory with PyTorch, the fastest-growing machine learning framework today, new strategies and insight are required. This paper first provides background on the topic, then presents a novel memory profiler for PyTorch along with results of it being used on real models. \par 

%%%%%%%%% BACKGROUND 
\section{Computational Machine Learning and Existing Profilers}
%%%%%%%%% BACKGROUND::Neural Networks
\cvprsubsection{Neural Networks}
While there are many types of artificial neural networks, they all involve the transformation of some input data (for example the pixels of an image) into meaningful output - in fact, a neural network is nothing more than a composition of mathematical functions. The function modeled by a neural network is special, however, because it can model \textit{any} mathematical function with arbitrary accuracy (provided that the network has at least two layers) \cite{dnn_history}. It is because of this property that neural networks have been labeled \enquote*{universal approximators}. Loosely, this means that if there are useful patterns in the world - whether they correspond to user preferences in movies based on past movies they have watched, or classification of images based on their pixel composition, or even cardiac arrest likelihood based on heart rate - a sufficiently-trained neural network can identify it. The effectiveness of a neural network in identifying these patterns depends primarily on two things: the amount of data it has been trained with, and its specific topology.
\par

\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth,height=9cm]{neural_network_machinelearningmastery.png}
\captionsetup{width=0.7\linewidth}
\caption{ Topology of a fully-connected feedforward neural network \cite{feedforward_pic}}
\label{fig:feedforwardNN}
\end{figure}

The neural network in Figure \ref{fig:feedforwardNN} shows the basic building blocks from which modern neural networks emerge. It is a simple feedforward neural network - a more complete list of network types can be found in Appendix \ref{appendix:types_of_networks}. The green nodes represent the input data to the network, which is what the network must use to make meaningful predictions. Perhaps, the input vector $[x_1,x_2,x_3,x_4]^T$ might represent a four-word sentence.\par

The gray lines in the diagram represent \textit{weights}, or the scaling coefficients from one node to the next. These weights together create linear combinations of the outputs of one layer into the nodes of the next layer. \par 

The purple nodes represent \textit{neurons}, which take a weighted sum of input nodes and produce a scalar-valued output. Each neuron is a nonlinear function; the nonlinearity is important here because without it the universal approximator guarantees no longer apply. For example, each neuron might be the ReLU function $f(x)=max(x,0)$ which clips all negative values. The intermediate outputs of each layer are known as \textit{feature maps}. Notice how there are two layers of purple nodes here - this denotes that the model has two hidden layers. There can be an arbitrary number of hidden layers in a network, and while there is no standard for the number of layers required for a network to be considered a \enquote*{deep neural network}, it should have at least three. \par 

Finally, the red nodes are the \textit{outputs}, which are the network's predictions based on the provided input data. In this example, each output node might be the one-hot-encoded representation of who likely wrote the input sentence. The result closest to 1 here indicates a network's predicted output node. If $y_1$ is associated with Alice, $y_2$ with Bob, and $y_3$ with Charlie, then the output vector $[y_1,y_2,y_3]^T = [0.983,0.217,0.015]^T$ indicates that it was likely Alice's sentence.

%%%%%%%%% BACKGROUND::GPUs and their application to DNNs
\cvprsubsection{GPUs and Their Applications to DNNs}
Graphics Processing Units (GPUs) are at the center of recent machine learning breakthroughs. It turns out that the computational workloads of neural networks are, in essence, sequences of matrix operations; as it was shown in the previous section, matrix multiplication is used to conveniently denote the application of scalar weights to the output of a neural network layer. GPUs can perform these computations orders of magnitude faster than CPUs due to the parallelism inherent in their design.  Figure \ref{fig:turing} shows the architecture of a modern GPU, which contains numerous computational cores (green).

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{Turing_TU104_chip_diagram.png}
\captionsetup{width=0.8\linewidth}
\caption{Diagram of the NVIDIA Turing TU104 architecture. Green sections are CUDA cores, which together compose Streaming Multiprocessors (SMs) \cite{turing_architecture}}
\label{fig:turing}
\end{figure}

To understand the value that GPUs bring in terms of computational parallelism, suppose that a simple program is scaling a matrix as follows:
\[
3
\begin{bmatrix}
    a  &  b      \\
    c  &  d      
\end{bmatrix} 
=
\begin{bmatrix}
    3a  &  3b      \\
    3c  &  3d      
\end{bmatrix} 
\]
This computation can be distributed over 4 GPU cores, which would each scale one element of the matrix. The program can then accumulate the results of the four operations, and store the results either by overwriting the old matrix or by creating a new one in memory. While this is a small example, the same principles can be applied to matrices with thousands of elements - as the dimensions increase, so does the benefit of using GPUs. 
\par

NVIDIA conveniently provides the functionality to distribute a program's workload across GPU cores in the form of a rich C++ API \cite{cuda_guide}. This API can be integrated seamlessly into existing codebases in such a way that the programmer is abstracted away from the low-level architecture of the GPU they are using. In recent years, highly-tuned primitives for machine learning-specific applications (e.g. convolution, softmax, batch normalization, neuron activations, etc.) have become publicly available \cite{cudnn}, which further allow programmers to use GPUs to their benefit.
\par 

%%%%%%%%% BACKGROUND::Inference and Training
\cvprsubsection{Inference and Training}
Neural networks are used in two modes: \textit{training} and \textit{inference}. While these two modes are closely related, they have certain key differences which ultimately affect the way they each interact with GPUs. A useful neural network must first be trained - this process involves iteratively improving the network's trainable parameters (i.e. weights) which are either initialized randomly or all to zero. Training can further be broken down into two steps: a \textit{forward pass} and a \textit{backward pass}. As it turns out, the forward pass is very similar to inference. Thus, the training of a neural network can be thought of as an extension of the inference process.
\par 

Both inference and the forward pass of training accept some input data (in the Figure \ref{fig:feedforwardNN} example this would be $[x_1,x_2,x_3,x_4]^T$). These input nodes are passed through the network - whatever its specific topology may be - and transformed through compositions of weight scaling and nonlinear activation functions. Then, at the end of the network the outputs are produced. For inference, these final outputs are almost always the only thing that the user is interested in, since they are the meaningful interpretations of the input data. For the forward pass of training, however, more than just the final outputs must be kept; in fact, nearly all of the intermediate values that were used to propagate the data through the network must be kept for later stages of the training algorithm.
\par

%%%%%%%%% BACKGROUND::Backpropagation and Memory
\cvprsubsection{Backpropagation and Memory}
Once the forward pass of training has finished, the first step is to evaluate the outputs of the network based on how accurate their predictions were. These evaluations are done via a \textit{loss function}, which quantifies the difference between the network's outputs and the \textit{ground truth} of the input data (in most cases this refers to the \textit{correct} interpretation of the input data). Referring once again to the previous example, if it was known that Bob in fact wrote the sentence, then the network's output $[y_1,y_2,y_3]^T = [0.983,0.217,0.015]^T$ would be compared to the ground truth $[0,1,0]^T$ and produce a large loss function result, since $y_2$ was not close to 1 and $y_1$ was incorrectly close to 1. While loss functions are beyond the scope of this paper, more information on them can be found in Appendix \ref{appendix:types_of_loss_functions}.
\par

Next comes perhaps the most important part of training - backpropagation. Originally gaining traction in the late 1980s \cite{hinton_backpropagation}, backpropagation involves calculating the partial derivatives of the loss function with respect to each weight in the network by using the chain rule. These calculations require the intermediate outputs of each layer, which are generally kept in memory during the forward pass and held until they are needed. Once all the gradients in the network have been calculated, they can then be adjusted appropriately. It is this extra space requirement that lies at the heart of why GPU memory is an essential resource. As for how the weights are updated, there are many different algorithms known as \textit{optimizers} available \cite{optimizers} which can be employed. Gradient Descent, perhaps the most ubiquitous optimization algorithm, is known as a first order optimization algorithm. It involves updating each weight in the network by a global scaling factor after all the gradients have been computed. Another popular algorithm is Adam \cite{adam}, which is known as a second order algorithm. Once again, the exact details of such algorithms are beyond the scope of this paper.
\par

The training process is done with a training dataset, which should be an unbiased representation of what actual data looks like in the real world. For example, it is important that a speech to text model is not trained using only the speech of one person, otherwise the network would learn to recognize \textit{their} speech very well but would likely under perform when fed data from another person's speech. Training datasets are generally very large - for example, a commonly-used object detection dataset known as COCO \cite{COCO} has 330 000 images, and is 37.57 GB in size. This is far beyond the size of a typical GPU's memory. One implementation of Gradient Descent involves using mini-batches, which are uniform-size subsets which together comprise the entire dataset. Each example within a mini-batch has the training algorithm applied to it, and only once the entire mini-batch is completed are the weights updated based on the sum of all gradients. A breakdown of the entire training process is summarized in Figure \ref{fig:training_memory_breakdown}.
\par

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{training_memory_breakdown.png}
\captionsetup{width=0.7\linewidth}
\caption{A breakdown of the training process, courtesy of TBD \cite{tbd}}
\label{fig:training_memory_breakdown}
\end{figure}

%%%%%%%%% BACKGROUND::Machine Learning Frameworks and Memory Profilers
\cvprsubsection{Machine Learning Frameworks and Memory Profilers}
\label{sec:Machine_Learning_Frameworks_and_Memory_Profiler}
In practice, DNNs are implemented via frameworks such as  Caffe \cite{caffe}, Theano \cite{theano}, MXNet \cite{mxnet_paper}, TensorFlow \cite{tensorflow}, CNTK \cite{cntk}, Chainer \cite{chainer}, Torch \cite{torch}, Keras \cite{keras}, and PyTorch \cite{pytorch_paper}. These frameworks provide convenient interfaces which enable researchers to codify their DNN architecture in common programming languages including Python, C++, and Java. Despite the rise in popularity of these frameworks in recent years, comprehensive tools to understand their memory usage are not provided out of the box. The EcoSystem research team at UofT, led by Prof. Gennady Pekhimenko, has recently developed open-source memory profiling tools for MXNet \cite{mxnet_paper}, TensorFlow \cite{tensorflow}, and CNTK \cite{cntk}; such a tool for PyTorch, however, remains to be created. PyTorch is increasingly becoming one of the most widely-used frameworks and thus a memory profiler would have a large impact to the community.
\par

All of these machine learning frameworks employ directed acyclic \textit{computational graphs}, which organize sequences of operations on data. The nodes of computational graphs are the program's variables (tensors, scalar values, etc.), while the edges represent data dependencies. Edges terminating at a node signify inputs to a certain mathematical function, whose output value is stored in the variable representing the node. For the purposes of backpropagation, each node must also know how to compute the derivative of its output with respect to its inputs. In Figure \ref{fig:computational_graph}, the green edges represent the flow of data in a forward pass. Once the output $e$ has been calculated, the backward pass can be initiated, during which the gradients are calculated by traversing the graph in the reverse direction (red edges).
\par 

\begin{figure}[ht]
\centering
\includegraphics[width=.7\textwidth]{computational_graph_example.png}
\captionsetup{width=0.7\linewidth}
\caption{A simple computational graph \cite{computational_graph_example}}
\label{fig:computational_graph}
\end{figure}

The frameworks that these memory profilers were created for all use static computational graphs. MXNet, for example, allows users to define computational symbols in their front end language which are then compiled under the hood. Since this graph is defined at the start, the program does not need to wait until the forward pass to create the graph. This compiled graph contains all placeholders for input and output tensors. When it comes time to use the graph - i.e. pass some input data through the network in the forward pass - MXNet internally calls its \textit{executor} \cite{mxnet_executor}. The executor will then allocate the memory for the tensors required for the graph. This point is important because specific memory regions of the GPU are linked to parts of the computational graph, and remain linked for the duration of the program. To derive meaning from what each part of the computational graph does - for example the part of the graph representing a Long Short-Term Memory (LSTM) cell \cite{lstm} within a speech recognition model - the naming employed by the programmer is used. Listing \ref{listing:lstm_example} shows an example of how a programmer might define an LSTM cell in their code. Notice the name assignments that are inputted as function parameters - i2h meaning \enquote{input to hidden} and h2h meaning \enquote{hidden to hidden} - which are meaningful descriptors of the neural network's layers.

\begin{lstlisting}[language=Python, caption={Symbolic naming of an LSTM cell, taken from the MXNet GPU memory profiler documentation \cite {mxnet_profiler}}, label={listing:lstm_example}]
i2h = symbol.FullyConnected(data=inputs, weight=self._iW, bias=self._iB,
                            num_hidden=self._num_hidden*4,
                            name='%si2h'%name)
h2h = symbol.FullyConnected(data=states[0], weight=self._hW, bias=self._hB,
                            num_hidden=self._num_hidden*4,
                            name='%sh2h'%name)
\end{lstlisting}

Thus, the picture so far is that a user defines their network and training/inference procedures in a front end language such as Python, using existing machine learning framework APIs. This framework interprets the user's code, and distributes the computational work to one or more GPUs (CPUs can be used as well, but for reasons outlined earlier GPUs are preferred). There are APIs used by the framework to interact with the GPU, which may vary as well. For NVIDIA GPUs, CUDA APIs are used, which interact with C++, C, and Fortran. For this reason, along with the fact that interpreted languages such as Python are generally less efficient than compiled languages, machine learning frameworks are themselves primarily written in C or C++.
\par 

For the purposes of this paper, one CUDA function is of particular importance: \texttt{cudaMalloc()}. This function allows the framework to request any number of bytes in memory on the device\footnote{Typically, \textit{device} refers to the GPU and \textit{host} refers to the CPU}. Each call to \texttt{cudaMalloc()} will return a pointer to where the requested memory lies; if space for a tensor was requested, then this pointer might be used for the remainder of the program to access and modify this tensor. However, it is ultimately up to the framework to manage its memory, and different frameworks have different strategies. 
\par 

The MXNet memory profiler's main challenge is linking symbol names to specific \texttt{cudaMalloc()} calls. This linkage allows the profiler to establish how much memory was needed for which part of the computational graph, and therefore can determine the memory breakdown of the models' weights, feature maps, and gradients. However, there is no direct way to do this in the existing MXNet codebase, and thus it was modified to make these connections. Function signatures were changed so that the strings representing symbol names could be passed down to the underlying allocations. A C++ helper class was created to output entries into a log file with information about the memory breakdown. The log file can then be parsed with provided scripts to create visualizations of the results.
\par 

\begin{figure}[ht]
\centering
\includegraphics[width=.8\textwidth]{mxnet_profiler_design.png}
\captionsetup{width=0.7\linewidth}
\caption{MXNet GPU memory profiler system design \cite{mxnet_profiler}}
\label{fig:mxnet_design}
\end{figure}

%%%%%%%%% METHODS
\section{Designing a PyTorch Memory Profiler}
\label{progress_to_date}
Unlike the frameworks for which these memory profilers were created, PyTorch uses completely different memory management and computational graph strategies. In order to create a memory profiler for PyTorch, these differences must be investigated and thoroughly understood. This involves investigating both its front end (Python\footnote{While PyTorch does have a C++ front end as well, it is generally less user-friendly and not used as often}) and back end (C++) code implementations. This section discusses the internals of PyTorch and how the memory profiler was created.

%%%%%%%%% METHODS::Memory Caching
\cvprsubsection{Memory Caching}
\label{sec:memory_caching}
The first difference - and perhaps most important one in the context of this paper - is how PyTorch handles its device memory. Unlike other frameworks, PyTorch uses \textit{caching} as its primary memory management strategy. The reason for this lies in the fact that PyTorch does not create a static computational graph with pre-allocated room for variables and tensors. This new paradigm, which is discussed in section \ref{dynamic_computational_graphs_and_autograd}, is known as \textit{dynamic computational graphs}. For now it suffices to say that PyTorch does not know how the forward and backward pass will be executed until runtime, i.e. until training or inference begins. Caching is best explained by the creators of PyTorch \cite{pytorch_paper}:

\begin{quote}
\enquote{
PyTorch implements a custom allocator which incrementally builds up a cache of CUDA memory and reassigns it to later allocations without further use of CUDA APIs. The incremental allocation is also crucial for better interoperability, because taking up all GPU memory ahead of time would prevent the user from utilizing other GPU-enabled Python packages. To further improve its effectiveness, this allocator was tuned for the specific memory usage patterns of deep learning. For example, it rounds up allocations to multiples of 512 bytes to avoid fragmentation issues. Moreover, it maintains a distinct pool of memory for every CUDA stream (work queue).
}
\end{quote}

By incrementally building up a cache of memory as-needed during runtime, PyTorch does not need to make frequent \texttt{cudaMalloc()} calls to reserve space for each tensor as the forward or backward pass progress. Instead, it reserves chunks of space which can be assigned and unassigned freely by PyTorch. The allocation strategy is relatively straightforward: when a tensor is needed by the program, it can request space for it using an allocator. If there is enough space in the existing cache, it takes memory from there. If not, the cache size is increased, and then the tensor is assigned memory from the cache.
\par 

In general, deallocation strategies are more nuanced than their allocation counterparts. There is no reason for a tensor to occupy space in device memory if the tensor is no longer being used by the program. Other machine learning frameworks generally use \textit{garbage collection} techniques to free memory once it is no longer needed. With this technique, the program will periodically analyze the state of all objects in its memory. All objects currently in use - i.e. tensors that are needed for future computations - are marked. The garbage collector will then free all objects in the address space that are not marked; these objects are of no use to the program. The issue with garbage collection is the timing of the collection - for the duration between a tensor becoming unneeded and garbage collection occurring there is wasted device memory.
\par 

PyTorch does not use garbage collection. Instead, to determine whether a certain tensor is needed or not, a technique called \textit{reference counting} is employed. The reference counter of a tensor is modified dynamically at runtime based on how many variables reference it. This system tracks both the back end references (PyTorch's internal references to the tensor) as well as the front end references (user-created references in their Python code) to ensure that a tensor is only freed when there is truly no use for it anymore. When a reference counter reaches zero, the memory occupied by the tensor is immediately released back to the cache. This means that at no point will there be objects with zero references which still occupy device memory.\par 

By gradually increasing the cache size as the program progresses, the majority of the CUDA memory API calls will occur near the start of the program, particularly in the first few iterations\footnote{An iteration refers to the combined forward and backward pass of one mini-batch through the network} of training. This is illustrated in Figure \ref{fig:cuda_memory_kernels}, where the utilization of the GPU (density of CUDA kernel execution in blue) is low in the first iteration compared to subsequent iterations. This decreased computational efficiency can be attributed to the costly memory management APIs.
\begin{figure}[ht]
\centering
\includegraphics[width=.9\textwidth]{cuda_kernel_caching.png}
\captionsetup{width=0.7\linewidth}
\caption{ResNet-50 GPU execution traces in PyTorch \cite{pytorch_paper}}
\label{fig:cuda_memory_kernels}
\end{figure}

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%     METHODS ::Tensors in PyTorch
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cvprsubsection{Tensors in PyTorch}
\label{sec:tensors_in_pytorch}
Thus far, tensors have been mentioned but not yet explained in detail. Tensors are n-dimensional data structures which contain a certain type of data, for example integers or floats. The actual data represented by a tensor - for example a matrix representing the scalar weights from the first to the second hidden layer of a network - are accompanied by some metadata which describes various attributes about the tensor. The metadata of a tensor includes its dimensions (depth, height, width), strides, element data type, and the device it is stored on. Among these, strides may be an unfamiliar one - in fact, striding is one of the defining features of PyTorch. To explain striding, it must first be understood that every tensor is associated with a \textbf{storage} object. A tensor can be thought of as the logical view of data, while a storage can be thought of as a structure which manages the raw data. It is entirely possible in PyTorch for two tensors to point to the same storage object, with each of the two tensors having different perspectives about how to view the underlying raw data. For example, suppose a PyTorch program created the following two tensors:

\[
X =
\begin{bmatrix}
    1      \\
    2      \\
    3      \\
    4      \\
\end{bmatrix} 
,
Y =
\begin{bmatrix}
    1  &  2      \\
    3  &  4      
\end{bmatrix} 
\]
Both $X$ and $Y$ contain the integers from 1 to 4; however, they each provide different logical representations of the data. This information is captured by the stride of a tensor, which are offsets to map element indices into specific memory addresses. Storages don't understand the data they encapsulate, it is up to the tensors to provide the interpretation. The reference counters explained in section \ref{sec:memory_caching} are in fact handled by storage objects. When no tensors point to a certain storage anymore, the memory occupied by the storage is release to the cache.  
\par 

Given that this paper is concerned with the memory usage of PyTorch, two questions must be answered:
\begin{enumerate}[label=(\roman*)]
  \item How can a tensor's link to underlying device memory be ascertained, and how can a memory profiler safely handle two or more tensors which point to the same underlying storage?
  \item Given (i), how can this link be translated into a specific size in bytes, and how can a memory profiler take into account PyTorch's unique caching memory management strategy, where CUDA memory allocations are no longer directly tied to a specific object within the framework?
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%     METHODS::Models in PyTorch
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cvprsubsection{Models in PyTorch}
\label{sec:models_in_pytorch}
Tensors can only get a user so far. Ultimately, machine learning researchers need an interface to codify their machine learning models, which can then be trained and used. The main interface used to do this in PyTorch is the \texttt{nn.Module} class. For example, Listing \ref{listing:nn.module_example} defines a simple two-layer network:
\par 
\begin{lstlisting}[language=Python, caption={Definition of a simple two-layer neural network from the PyTorch documentation \cite{pytorch_tutorial}}, label={listing:nn.module_example}]
class TwoLayerNet(torch.nn.Module):
    def __init__(self, D_in, H, D_out):
        super(TwoLayerNet, self).__init__()
        self.linear1 = torch.nn.Linear(D_in, H)
        self.linear2 = torch.nn.Linear(H, D_out)

    def forward(self, x):
        h_relu = self.linear1(x).clamp(min=0)
        y_pred = self.linear2(h_relu)
        return y_pred
\end{lstlisting}
Here, a network called \texttt{TwoLayerNet} is being defined. Notice how it inherits from \texttt{nn.Module}. The \texttt{\_\_init\_\_()} method of this network is invoked whenever an instance of it is created and a user generally instantiates one model which persists for the duration of their PyTorch program. This method is where the \textit{structure} of the model is defined - how many and what types of layers are present, which layers connect to what, and so on. The code here can be as complex or as simple as the user wishes; this modularity is what enables PyTorch to implement arbitrary neural networks. This example creates two linear layers using the PyTorch-provided \texttt{torch.nn.Linear} \footnote{The \texttt{torch.nn.Linear} layer applies the linear transformation $y=xA^T + b$} implementations \cite{pytorch_docs}. The input parameters \texttt{D\_in} and \texttt{D\_out} are used to specify the dimensions of these two layers.
\par 

One interesting feature of the \texttt{\_\_init\_\_()} method is that all \textit{learnable} layers of the network - i.e. ones that must be trained, which must have their gradients computed - are automatically registered and saved to the model's list of \texttt{parameters}. Using the \texttt{torch.nn.Linear} API handles this by default, but if one were to define a learnable tensor within a model themselves, they would have to wrap them with \texttt{torch.nn.Parameter}. For example, observe the following two tensor definitions: 

\begin{lstlisting}[language=Python]
import torch
class MyNetwork(torch.nn.Module):
    def __init__(self):
        super(MyNetwork, self).__init__()
        self.wont_be_registered=torch.tensor([1.0])
        self.will_be_registered=torch.nn.Parameter(torch.tensor([1.0]))
\end{lstlisting}
Here, one tensor is defined correctly and thus will forever be saved in a special data structure by the PyTorch back end code. This data structure is called the \texttt{state\_dict}. The other is not, and will not be added to the \texttt{state\_dict}. Wrapping a tensor in this way does not change any of its functionality. To verify that tensors are being saved as expected, an instance of the model is instantiated and then a built-in class function called \texttt{named\_parameters()} is invoked to reveal the named items within the model's \texttt{state\_dict}:
\begin{lstlisting}[language=Python]
model=MyNetwork()
for name,param in model.named_parameters():
    print(f"Tensor name={name}, Tensor data={param}")
\end{lstlisting}
The output is as follows:
\begin{lstlisting}[language=Python]
>>> Tensor name=will_be_registered, Tensor data=Parameter containing:
    tensor([1.], requires_grad=True)
\end{lstlisting}
\par
This convenient saving of layer names has many similarities to the one previously discussed in MXNet in section \ref{sec:Machine_Learning_Frameworks_and_Memory_Profiler}, and will serve to be useful in creating a new memory profiler for PyTorch given the mapping it provides between layer names and their data objects. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%     METHODS::Dynamic Computational Graphs and Autograd
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cvprsubsection{Dynamic Computational Graphs and Autograd}
\label{dynamic_computational_graphs_and_autograd}
The network definition in Listing \ref{listing:nn.module_example} has one other important definition, namely its \texttt{forward()} method. This method describes the exact sequence of calculations that will be undertaken during the forward pass of the network, both during training and inference. This example is trivial, and the backward pass procedure could be defined manually with some basic calculus and a few lines of code. However, modern neural networks are very complex and have detailed computational procedures that define their forward passes. Manually defining a backward pass each time would be an arduous and error-prone process, and thus a more scalable system is required.
\par

PyTorch uses \textbf{automatic differentiation} \cite{automatic_differentiation}, implemented within the \texttt{torch.autograd} package, to take care of this problem. Distinct from other algorithmic methods such as symbolic and numerical differentiation, automatic differentiation is founded on the observation that any arbitrary computer program can be decomposed into arithmetic operations ($+,-,*,\div$) and elementary functions ($log, sine, cosine, exp,$ etc.). Recursively applying the chain rule allows the program to calculate derivatives (subject of course to floating point precision errors) using only an order of $O(n)$ extra computation. In practice, the linear factor is typically small in magnitude, leading to only a small decrease in performance.
\par 

During each iteration, PyTorch will gradually create a computational graph \textit{dynamically} as the program executes - this is a large departure from other frameworks, such as MXNet, which create a \textit{static} graph at the start of the program. When PyTorch starts the backward pass with a $.backward()$ call, the graph will be walked back and gradients will be calculated. Returning to Figure \ref{fig:computational_graph}, the gradient of the output node $e$ with respect to the input node $b$ could not be calculated directly; because of the nature of the chain rule, the gradients of \textit{intermediate} nodes were required for the calculation. These occupy space in memory and must not be neglected. Notice also that derivatives with respect to $a$ were \textit{not} calculated; for this reason, it cannot be assumed that every node will participate in PyTorch's backward pass procedure. In fact, if the gradient of $a$ with respect to the output $e$ were not required, and there were other intermediate nodes in the graph which were not needed for computing the gradient of $e$, these nodes would not have their gradients calculated either. Thus, it cannot be assumed that every tensor involved in a PyTorch computational graph will necessarily have its gradients computed and stored in memory. More generally, if a tensor has \texttt{requires\_grad=True}, then PyTorch must calculate gradients for it. The exact behaviour is outlined in the PyTorch documentation \cite{pytorch_doc_leaf_requires_grad}. 
\par 

%%%%%%%%% METHODS::Feature Maps, Gradients, and Hooks
\cvprsubsection{Feature Maps, Gradients, and Hooks}
Feature maps are the scalar outputs of neurons, and gradients are the derivatives necessary for backpropagation. To create a memory profiler, the tensors which represent these data structures must be properly accounted for. During the forward pass of training, feature maps are gradually computed and stored. Then, during the backward pass, gradients are computed using these feature maps. Immediately after the backward pass, all of these intermediate tensors are no longer needed, and are cleared. At this point, only the gradients of \textbf{leaf tensors} \cite{pytorch_doc_leaf_requires_grad} of the computational graph - i.e. the ones ones which must be improved during the iteration - are kept in memory. These gradients can then be used by an optimizer (e.g. Adam) to take a step in the direction which reduces the loss function of the network.
\par 

This complicated set of procedures - where the total allocated memory routinely fluctuates throughout training, and where tensor memory is not even linked directly to \texttt{cudaMalloc()} calls but rather is simply taken and returned to a global cache - provides new challenges not faced when tracking the memory of a static graph framework. Luckily, however, PyTorch provides some hidden tools which help shed light on the forward and backward pass: \textbf{hooks}.
\par 

There are several variants of hooks in PyTorch, but unfortunately they have very little documentation. Still, the insight they bring is valuable, despite the fact that most information about them is only found spread across the PyTorch forums \cite{pytorch_forum}. Hooks are \textit{functions} which can be registered on either a \texttt{nn.Module} or a \texttt{Tensor} object. The three methods used to register hooks are: \texttt{register\_hook()}, \texttt{register\_forward\_hook()}, and \texttt{register\_backward\_hook()}. Depending on the type of hook, during the backward or forward pass these functions are called. Hooks allow direct access to the intermediate feature maps and gradients at the time of their creation - exactly what is needed for a memory profiler.
\par 

%%%%%%%%% PROGRESS TO DATE::Designing the Profiler
\cvprsubsection{Designing the Profiler}
In the face of all the internal complexities that have been outlined thus far, it should be clear that new mechanisms must be created to profile PyTorch's device memory usage. A proposed strategy to attain a layer-by-layer breakdown into weights, feature maps, and gradients is developed using knowledge and methods that have been developed in previous sections. It is noted that this strategy departs from the strategies used with existing profilers in that it only uses the front end Python API available in PyTorch. This means that the profiler is version-agnostic and thus future-proof. Given the particularly fast-changing nature of PyTorch, with several recent large-scale design changes such as the variable-tensor merger \cite{pytorch_variable_tensor_merger}, having a profiler that does not have to be re-coded each time a new update arrives is a significant benefit. 
\par 

First, let us return to the two questions posed in section \ref{sec:tensors_in_pytorch}. Question (i) requires linking tensors to their device memory. To solve this, there is a fundamental structure in PyTorch known as a \textbf{DataPtr}. Defined in \texttt{c10/core/Allocator.h} in the source code, these structures are described as follows:
\begin{quote}
\enquote{
A DataPtr is a unique pointer (with an attached deleter and some context for the deleter) to some memory, which also records what device is for its data.}
\end{quote}
The DataPtr of a tensor is saved in the \texttt{storage} object that the tensor points to, and can be accessed by calling \texttt{tensor.storage().data\_ptr()}. This also provides a way to determine if two tensors with different names actually share the same underlying memory, and thus should not be counted twice by the profiler. A hash table which maps the layer naming of a tensor to its DataPtr can be used by the profiler to keep track of tensors. It is important that this hash table \textbf{does not store a direct reference to the tensor itself}, since this would prevent the reference counting system from freeing it as expected, and thus increase the memory usage of the program. 
\par 

Question (ii) involves measuring the amount of memory used by a tensor. Unfortunately there is no direct way to do this, but a convenient calculation can be used instead. This is can be done by multiplying the number of elements by the size per element of a tensor. This information is spread out between the tensor and storage objects and can be computed as follows:
\begin{lstlisting}[language=Python]
total_memory = tensor.storage().size() * tensor.element_size()
\end{lstlisting}
The calculated total memory size can then be saved by the profiler and used by its algorithm. It is noted that, as mentioned in section \ref{sec:memory_caching}, allocations are rounded to 512 bytes and this must be taken into account by the profiler.
\par 

\begin{figure}[ht]
\centering
\includegraphics[width=1.0\textwidth]{profiler_system_diagram.PNG}
\captionsetup{width=0.9\linewidth}
\caption{System diagram of the proposed PyTorch memory profiler}
\label{fig:profiler_system_diagram}
\end{figure}

Figure \ref{fig:profiler_system_diagram} shows a system diagram of the PyTorch memory profiler. Blue steps comprise the normal training workflow, and orange steps are the modifications to the flow done autonomously by the profiler. After the machine learning model has been has been instantiated by the training program, a \texttt{memory\_profiler} instance is created. This object contains all the functionality necessary for the memory usage to be profiled. When it is initialized, it will analyze the layers of the model and save their data pointers. The layer names are extracted as well, which provide meaningful descriptions of their use. \par 

Next, the profiler registers forward and backward hooks. These allow the profiler to inspect feature maps and gradients, respectively. The Autograd engine accumulates values internally in C++ buffers, which are only exposed via these hooks. The hooks that are registered are specialized functions defined within the profiler to gather memory consumption metrics. Importantly, the data pointers of feature maps and gradients are also collected during runtime to ensure that no intermediate tensor is mistakenly counted twice by the memory profiler. \par 

During the training algorithm, the forward and backward passes execute as normal. However, the hooks registered earlier are automatically run after every intermediate computation. The profiler keeps track of the memory usage by each part of the model, and reports relevant diagnostics. The cache is also monitored, and statistics about its peak size and its current size at the end of each iteration are reported. \par 

%%%%%%%%% PROFILER GUIDE
\section{Profiler Guide}
\label{profiler_guide}
This section walks through an example of how the PyTorch memory profiler is to be used. The profiler is available for use on GitHub \footnote{\href{https://github.com/izaakniksan/engsci\_thesis/tree/master/pytorch\_mem\_profiler}{https://github.com/izaakniksan/engsci\_thesis/tree/master/pytorch\_mem\_profiler}}, with detailed setup and usage instructions.

\cvprsubsection{Profiler Setup}
The prerequisites to use the memory profiler are: 
\begin{myitemize}
\item Python 3.6 or later
\item PyTorch (https://pytorch.org/)
\item CUDA (any version, as long as the training code uses it)
\end{myitemize}

One of the largest benefits of this memory profiler is that there is no lengthy installation process. Other existing profilers require downloading a specific version of the machine learning framework, then patching the source code, then building the library from source; this PyTorch profiler is not version-dependent and no re-installation of the library is required.

Once the GitHub repo has been cloned, add the path to the profiler to your path in your training code as follows:
\begin{lstlisting}[language=Python]
import sys
sys.path.append('<path to pytorch_mem_profiler/ directory>')
from pytorch_mem_profiler import *
\end{lstlisting}

\cvprsubsection{Profiler Usage}
There are only three steps (and only four lines of code) required to start using the profiler:
\begin{myitemize}
\item Create a profiler instance
\item Call \texttt{.record\_stats()} after every iteration
\item Call \texttt{.epoch\_end()} after every epoch
\end{myitemize}
The usage of the profiler will be demonstrated using the Neural Collaborative Filtering model as an example. Below is example code from training code \cite{tbd_suite} which instantiates a model. On line 14, the model is sent to the GPU.
\begin{lstlisting}[language=Python]
...
# Create model
model = NeuMF(nb_users, nb_items,
              mf_dim=args.factors, mf_reg=0.,
              mlp_layer_sizes=args.layers,
              mlp_layer_regs=[0. for i in args.layers])

# Add optimizer and loss to graph
optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)
criterion = nn.BCEWithLogitsLoss()

if use_cuda:
    # Move model and loss to GPU
    model = model.cuda()
    criterion = criterion.cuda()
...
\end{lstlisting}
Next, right before the training loop, an instance of the \texttt{memory\_profiler} class must be instantiated. The model is passed in as an input argument, along with two other optional arguments:
\begin{myitemize}
\item \textbf{print\_period} (postive integer) determines the number of iterations between memory reporting (default value is 1).
\item \textbf{csv} (boolean) allows profiling data to also be exported into a .csv file located in \texttt{./memory\_csv\_data/} (default value is False).
\end{myitemize}
The profiler initialized below will report statistics every 5 iterations to the terminal and to a csv file. The \texttt{global} keyword ensures that the profiler is accessible anywhere within the main training program.
\begin{lstlisting}[language=Python]
global profiler
profiler = memory_profiler(model, print_period=5, csv=True)
\end{lstlisting}
The final step is to call \texttt{.record\_stats()} at the end of each iteration and \texttt{.epoch\_end()} at the end of each epoch. In the following training loop, Lines 33 and 35 were added:
\begin{lstlisting}[language=Python]
# Epoch loop
for epoch in range(args.epochs):

    model.train()
    losses = utils.AverageMeter()

    begin = time.time()
    loader = tqdm.tqdm(train_dataloader)
    length = len(loader)
    
    # Iteration loop
    for batch_index, (user, item, label) in enumerate(loader):
        user = torch.autograd.Variable(user, requires_grad=False)
        item = torch.autograd.Variable(item, requires_grad=False)
        label = torch.autograd.Variable(label, requires_grad=False)
        if use_cuda:
            user = user.cuda(async=True)
            item = item.cuda(async=True)
            label = label.cuda(async=True)

        outputs = model(user, item)
        loss = criterion(outputs, label)
        losses.update(loss.data.item(), user.size(0))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Save stats to file
        description = ('Epoch {} Loss {loss.val:.4f} ({loss.avg:.4f})'
                       .format(epoch, loss=losses))
        loader.set_description(description)
        profiler.record_stats()

    profiler.epoch_end()
    hits, ndcgs = val_epoch(model, test_ratings, test_negs, args.topk,
                            use_cuda=use_cuda, output=valid_results_file,
                            epoch=epoch, processes=args.processes)
\end{lstlisting}
The results will be printed as the training progresses. Since PyTorch uses a memory caching strategy, tensors dynamically take and release from the GPU memory cache. The profiler will give you insight into the cache size, as well as a detailed layer-by-layer breakdown of what the memory is being used for. As the training is running, the csv output file located in \texttt{./memory\_csv\_data/} will become populated with memory diagnostics. Additionally, the following table will be printed to the screen:
\begin{verbatim}
*******************************************
Memory Usage for Iteration 85 of Epoch 1
*******************************************
Peak cached..........................857 MB
Current cached.......................857 MB
Total feature map usage..............301 MB

Total layer weight usage.............127 MB
  mf_user_embed.weight................35 MB
  mf_item_embed.weight.................7 MB
  mlp_user_embed.weight...............71 MB
  mlp_item_embed.weight...............14 MB
  mlp.0.weight.........................0 MB
  mlp.0.bias...........................0 MB
  mlp.1.weight.........................0 MB
  mlp.1.bias...........................0 MB
  mlp.2.weight.........................0 MB
  mlp.2.bias...........................0 MB
  final.weight.........................0 MB
  final.bias...........................0 MB

Total layer weight gradient usage....127 MB
  mf_user_embed.weight grad...........35 MB
  mf_item_embed.weight grad............7 MB
  mlp_user_embed.weight grad..........71 MB
  mlp_item_embed.weight grad..........14 MB
  mlp.0.weight grad....................0 MB
  mlp.0.bias grad......................0 MB
  mlp.1.weight grad....................0 MB
  mlp.1.bias grad......................0 MB
  mlp.2.weight grad....................0 MB
  mlp.2.bias grad......................0 MB
  final.weight grad....................0 MB
  final.bias grad......................0 MB
Intermediate gradients................34 MB
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%     RESULTS
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Profiling Results}
\label{profiling_results}
This section presents memory profiling results for the training of modern deep neural networks. All models presented here have open-source implementations and training code available on TBD Suite \cite{tbd_suite}. The system used for training benchmarks is described in Table \ref{table:system_specs}.

\begin{table}[H]
\centering
\resizebox{0.6\textwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Type} & \textbf{Component Used}                                  \\ \midrule
CPU           & AMD Ryzen 7 2700X, 4.3 GHz Max Boost, 8 Cores            \\ \midrule
GPU           & NVIDIA GeForce GTX 1080 Ti, 11 GB GDDR5X                 \\ \midrule
OS            & Ubuntu 16.04 LTS                                         \\ \midrule
Host Memory   & 16 GB DDR4, 3200 MHz                                     \\ 
\bottomrule
\end{tabular}}
\caption{Specification of system used for profiling}
\label{table:system_specs}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%     RESULTS :: Neural Collaborative Filtering (NCF)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cvprsubsection{Neural Collaborative Filtering (NCF) Training Benchmarks}
This section presents the memory breakdown for the training of a Neural Collaborative Filtering \cite{ncf} model with PyTorch. The MovieLens 20M dataset \cite{ml-20m} was used for input training data. The hyperparameters used for all training experiments are listed in Table \ref{table:ncf_hyperparameters}.


\begin{table}[H]
\centering
\resizebox{0.6\textwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Hyperparameter} & \textbf{Value}                                  \\ \midrule
Learning rate           & 0.0002            \\ \midrule
Number of predictive factors  & 64  \\ \midrule
Sizes of hidden layers for MLP  & 256, 256, 128, 64  \\ \midrule
Number of negative examples per interaction  & 8  \\ \midrule
Threshold  & 0.635  \\ \midrule
Number of processes  & 1  \\ \midrule
Number of dataloader workers  & 0 \\ 
\bottomrule
\end{tabular}}
\caption{Hyperparameters used for NCF memory profiling experiments}
\label{table:ncf_hyperparameters}
\end{table}

Figure \ref{fig:recommendation_bar_graph} shows the memory usage of the feature maps, layer weights, and layer gradients for different mini-batch sizes. Feature maps consume the majority of the GPU memory, which is especially evident as the mini-batch size increases. The memory usage of layer weights and the gradients of these layer weights are not dependent on the mini-batch size. Table \ref{table:ncf_full_breakdown} shows a more detailed memory breakdown. The intermediate gradients listed in the last row are tensors which were seen by the profiler's hooks during the backward pass, but whose memory was not attributable to the named layer weight gradients. The memory consumed by these intermediate tensors did increase with mini-batch size.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{recommendation_bar_graphs.PNG}
\captionsetup{width=0.8\linewidth}
\caption{Feature map, layer weight, and layer weight gradient memory usage of NCF training for different batch sizes}
\label{fig:recommendation_bar_graph}
\end{figure}


\begin{table}[H]
\centering
\resizebox{0.75\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|}
\hline
\backslashbox{\textbf{Type}}{\textbf{Batch Size}}
                                  & \textbf{32768 Ratings} & \textbf{65536 Ratings} & \textbf{131072 Ratings} \\ \hline
\textbf{peak cached}             & \textbf{857}                        & \textbf{1107}                       & \textbf{1714}                        \\ \hline
\textbf{total feature map usage} & \textbf{301}                        & \textbf{697}                        & \textbf{1451}                        \\ \hline
\textbf{total layer weight usage}     & \textbf{127}                        & \textbf{127}                        & \textbf{127}                         \\ \hline
mf\_user\_embed.weight            & 35                         & 35                         & 35                          \\ \hline
mf\_item\_embed.weight            & 7                          & 7                          & 7                           \\ \hline
mlp\_user\_embed.weight           & 71                         & 71                         & 71                          \\ \hline
mlp\_item\_embed.weight           & 14                         & 14                         & 14                          \\ \hline
mlp.0.weight                      & 0                          & 0                          & 0                           \\ \hline
mlp.0.bias                        & 0                          & 0                          & 0                           \\ \hline
mlp.1.weight                      & 0                          & 0                          & 0                           \\ \hline
mlp.1.bias                        & 0                          & 0                          & 0                           \\ \hline
mlp.2.weight                      & 0                          & 0                          & 0                           \\ \hline
mlp.2.bias                        & 0                          & 0                          & 0                           \\ \hline
final.weight                      & 0                          & 0                          & 0                           \\ \hline
final.bias                        & 0                          & 0                          & 0                           \\ \hline
\textbf{total layer weight gradient usage}     & \textbf{127}                        & \textbf{127}                        & \textbf{127}                         \\ \hline
mf\_user\_embed.weight grad       & 35                         & 35                         & 35                          \\ \hline
mf\_item\_embed.weight grad       & 7                          & 7                          & 7                           \\ \hline
mlp\_user\_embed.weight grad      & 71                         & 71                         & 71                          \\ \hline
mlp\_item\_embed.weight grad      & 14                         & 14                         & 14                          \\ \hline
mlp.0.weight grad                 & 0                          & 0                          & 0                           \\ \hline
mlp.0.bias grad                   & 0                          & 0                          & 0                           \\ \hline
mlp.1.weight grad                 & 0                          & 0                          & 0                           \\ \hline
mlp.1.bias grad                   & 0                          & 0                          & 0                           \\ \hline
mlp.2.weight grad                 & 0                          & 0                          & 0                           \\ \hline
mlp.2.bias grad                   & 0                          & 0                          & 0                           \\ \hline
final.weight grad                 & 0                          & 0                          & 0                           \\ \hline
final.bias grad                   & 0                          & 0                          & 0                           \\ \hline
\textbf{intermediate gradients}                & \textbf{34}                         & \textbf{101}                        & \textbf{203}                         \\ \hline
\end{tabular}}
\caption{Neural Collaborative Filtering memory breakdowns for varying batch sizes. All values rounded to the nearest MB.}
\label{table:ncf_full_breakdown}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%     RESULTS :: ResNet-50 Benchmarks
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cvprsubsection{ResNet-50 Training Benchmarks}
This section presents the memory breakdown for the training of a ResNet-50 v2 \cite{resnet} model on PyTorch. The ImageNet LSVRC 2012 dataset \cite{imagenet2012} was used for input training data. The hyperparemeters used for all training experiments are listed in Table \ref{table:resnet_hyperparameters}.

\begin{table}[H]
\centering
\resizebox{0.35\textwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Hyperparameter} & \textbf{Value}                                  \\ \midrule
Learning rate  & 0.1  \\ \midrule
Number of dataloader workers  & 4  \\ 
\bottomrule
\end{tabular}}
\caption{Hyperparameters used for ResNet-50 v2 memory profiling experiments}
\label{table:resnet_hyperparameters}
\end{table}

Figure \ref{fig:resnet_bar_graph} shows the memory usage of the feature maps, layer weights, and layer gradients for different mini-batch sizes. As was the case with the NCF model, again the feature maps consume the majority of the GPU memory. Given the large number of layers, a full layer-by-layer memory breakdown is presented in Appendix \ref{appendix:resnet_50_full_breakdown}.
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{resnet_bar_graphs.PNG}
\captionsetup{width=0.8\linewidth}
\caption{Feature map, layer weight, and layer weight gradient memory usage of ResNet-50 v2 training for different batch sizes}
\label{fig:resnet_bar_graph}
\end{figure}

Using existing memory profilers, the EcoSystem team has gathered memory breakdowns for ResNet-50 implementations in other frameworks. While the system specifications of those experiments are not identical to those used for the PyTorch experiments (notably, a Quadro P4000 GPU was used), the breakdowns are similar. This further demonstrates the accuracy of this proposed PyTorch memory profiler and its ability to correctly characterize training memory usage. The results for those breakdowns are shown in Figure \ref{fig:other_frameworks_resnet}.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{ecosystem_resnet_other_frameworks_bar_graph.PNG}
\captionsetup{width=0.8\linewidth}
\caption{Memory breakdowns of ResNet-50 training for other frameworks \cite{tbd_benchmarks}}
\label{fig:other_frameworks_resnet}
\end{figure}


%%%%%%%%% CONCLUSION
\section{Conclusion}
\label{conclusion}
In this paper, a novel memory profiling tool for PyTorch was proposed. Given the differences between how PyTorch handles GPU memory compared to other frameworks - namely its use of caching and dynamic computational graphs - new methods were employed to design and implement this profiler. Given the importance of GPU memory in the training of large and complex modern deep neural networks, and with the increasing popularity of PyTorch, a tool which provides insight into how memory is being used will benefit the community of researchers. Due to the recent rise of multi-GPU training, extending this profiler to provide a device-based breakdown of memory usage is an area for future work. As new models are created and implemented, it is hoped that this PyTorch memory profiler will shed light on new optimization techniques that can increase training efficiency and performance.

%%%%%%%%% APPENDIX
\newpage
\begin{appendices}


%%%%%% APPENDIX: Resnet 50
\section{Detailed ResNet-50 Memory Statistics}
\label{appendix:resnet_50_full_breakdown}
In this section the layer-by-layer PyTorch memory breakdown of ResNet-50 training for a batch size of 32 is presented. Training was done using system specs listed in Table \ref{table:system_specs}. All values are rounded to the nearest MB and were taken directly from the profiler's csv output. In the interest of space, only non-zero values are shown.
\begin{longtable}{@{}ll@{}}
\toprule
\textbf{peak cached}                              & \textbf{5475} \\ \midrule
\textbf{total feature map usage}                  & \textbf{5219} \\ \midrule
\textbf{total layer weight usage}                     & \textbf{102}  \\ \midrule
module.layer2.0.conv2.weight              & 1    \\ \midrule
module.layer2.0.downsample.0.weight       & 1    \\ \midrule
module.layer2.1.conv2.weight              & 1    \\ \midrule
module.layer2.2.conv2.weight              & 1    \\ \midrule
module.layer2.3.conv2.weight              & 1    \\ \midrule
module.layer3.0.conv1.weight              & 1    \\ \midrule
module.layer3.0.conv2.weight              & 2    \\ \midrule
module.layer3.0.conv3.weight              & 1    \\ \midrule
module.layer3.0.downsample.0.weight       & 2    \\ \midrule
module.layer3.1.conv1.weight              & 1    \\ \midrule
module.layer3.1.conv2.weight              & 2    \\ \midrule
module.layer3.1.conv3.weight              & 1    \\ \midrule
module.layer3.2.conv1.weight              & 1    \\ \midrule
module.layer3.2.conv2.weight              & 2    \\ \midrule
module.layer3.2.conv3.weight              & 1    \\ \midrule
module.layer3.3.conv1.weight              & 1    \\ \midrule
module.layer3.3.conv2.weight              & 2    \\ \midrule
module.layer3.3.conv3.weight              & 1    \\ \midrule
module.layer3.4.conv1.weight              & 1    \\ \midrule
module.layer3.4.conv2.weight              & 2    \\ \midrule
module.layer3.4.conv3.weight              & 1    \\ \midrule
module.layer3.5.conv1.weight              & 1    \\ \midrule
module.layer3.5.conv2.weight              & 2    \\ \midrule
module.layer3.5.conv3.weight              & 1    \\ \midrule
module.layer4.0.conv1.weight              & 2    \\ \midrule
module.layer4.0.conv2.weight              & 9    \\ \midrule
module.layer4.0.conv3.weight              & 4    \\ \midrule
module.layer4.0.downsample.0.weight       & 8    \\ \midrule
module.layer4.1.conv1.weight              & 4    \\ \midrule
module.layer4.1.conv2.weight              & 9    \\ \midrule
module.layer4.1.conv3.weight              & 4    \\ \midrule
module.layer4.2.conv1.weight              & 4    \\ \midrule
module.layer4.2.conv2.weight              & 9    \\ \midrule
module.layer4.2.conv3.weight              & 4    \\ \midrule
module.fc.weight                          & 8    \\ \midrule
                                          &      \\ \midrule
\textbf{total layer weight gradient usage}           & \textbf{94}  \\ \midrule
module.layer2.0.conv2.weight\_grad        & 1    \\ \midrule
module.layer2.0.downsample.0.weight\_grad & 1    \\ \midrule
module.layer2.1.conv2.weight\_grad        & 1    \\ \midrule
module.layer2.2.conv2.weight\_grad        & 1    \\ \midrule
module.layer2.3.conv2.weight\_grad        & 1    \\ \midrule
module.layer3.0.conv1.weight\_grad        & 1    \\ \midrule
module.layer3.0.conv2.weight\_grad        & 2    \\ \midrule
module.layer3.0.conv3.weight\_grad        & 1    \\ \midrule
module.layer3.0.downsample.0.weight\_grad & 2    \\ \midrule
module.layer3.1.conv1.weight\_grad        & 1    \\ \midrule
module.layer3.1.conv2.weight\_grad        & 2    \\ \midrule
module.layer3.1.conv3.weight\_grad        & 1    \\ \midrule
module.layer3.2.conv1.weight\_grad        & 1    \\ \midrule
module.layer3.2.conv2.weight\_grad        & 2    \\ \midrule
module.layer3.2.conv3.weight\_grad        & 1    \\ \midrule
module.layer3.3.conv1.weight\_grad        & 1    \\ \midrule
module.layer3.3.conv2.weight\_grad        & 2    \\ \midrule
module.layer3.3.conv3.weight\_grad        & 1    \\ \midrule
module.layer3.4.conv1.weight\_grad        & 1    \\ \midrule
module.layer3.4.conv2.weight\_grad        & 2    \\ \midrule
module.layer3.4.conv3.weight\_grad        & 1    \\ \midrule
module.layer3.5.conv1.weight\_grad        & 1    \\ \midrule
module.layer3.5.conv2.weight\_grad        & 2    \\ \midrule
module.layer3.5.conv3.weight\_grad        & 1    \\ \midrule
module.layer4.0.conv1.weight\_grad        & 2    \\ \midrule
module.layer4.0.conv2.weight\_grad        & 9    \\ \midrule
module.layer4.0.conv3.weight\_grad        & 4    \\ \midrule
module.layer4.0.downsample.0.weight\_grad & 8    \\ \midrule
module.layer4.1.conv1.weight\_grad        & 4    \\ \midrule
module.layer4.1.conv2.weight\_grad        & 9    \\ \midrule
module.layer4.1.conv3.weight\_grad        & 4    \\ \midrule
module.layer4.2.conv1.weight\_grad        & 4    \\ \midrule
module.layer4.2.conv2.weight\_grad        & 9    \\ \midrule
module.layer4.2.conv3.weight\_grad        & 4    \\ \midrule
\textbf{intermediate gradients}                       & 26   \\ \bottomrule
\end{longtable}
\newpage


\section{Types of Neural Networks}
\label{appendix:types_of_networks}
\begin{figure}[ht]
\centering
\includegraphics[width=0.75\textwidth]{types_of_NN.png}
%\captionsetup{width=0.7\linewidth}
%\caption{ \cite{types_of_NN_pic}}
\end{figure}
\begin{center}A guide to common types of neural networks, created by van Vleen \cite{types_of_NN_pic}.\end{center}
\newpage

\section{Types of Loss Functions}
\label{appendix:types_of_loss_functions}
\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{loss_functions.png}
\end{figure}
\begin{center}A guide to loss functions, created by Janocha and Czarnecki \cite{loss_functions_paper}.\end{center}
\newpage


\end{appendices}





%% TODO: get list of loss and activation functions and add them to appendix, then \ref them in the paper



%%%%%%%%% REFERENCES
\newpage
\printbibliography
\end{document}
